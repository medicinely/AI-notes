# 1. tensorflow结构

tv远程：1 398 601 430

tensor（数组）：张量

operation（op）：专门运算的操作节点，所有操作都是一个op

graph（图）：你整个程序的结构

会话：运算图的程序

## 1.1 图

> 图(包含一组op和tensor),上下文环境

* 图默认已经注册，一组表示tf.Operation计算单位的对象和tf.Tensor表示操作之间流动的数据单元的对象

  * op：只要使用tensorflow的API定义的函数都是op
  * tensor：指代的是数据

* 获取调用

  * tf.get_default_graph()
  * op、sess或者tensor的graph属性

* 图的创建

  * ```python
    g = tf.Graph()
    with g.as_default():
    	a = tf.constant(1.0)
    	assert c.graph is g	
    ```

## 1.2 会话

* tensortlow

  * API   <---->   回话  <---->  系统+(cpu、gpu)
  * 一个会话只能运行一个图

  > 会话：1.运行图的结构
  >
  > ​			2.分配资源计算
  >
  > ​			3.掌握资源（变量资源，队列，线程）

  * 前端系统：定义程序的图结构（API）
  * 后端系统：运算图结构

* tf.Session()

  * 运行Tensorflow操作图的类，使用默认注册的图（可指定运行图）

* 会话资源

  > 会话可能拥有很多资源，如 tf.Variable，tf.QueueBase和tf.ReaderBase，会话结束后需要进行资源释放

  * 方式一：
    * sess = tf.Session	sess.run()	sess.close()
  * 方式二：使用上下文管理器
    * with tf.Session() as sess:  sess.run()
  * 显示图的会话内部调用的资源
    * config = ft.ConfigProto(log_device_placement=True)
  * 交互式的写法
    * tf.InteractiveSave()

### 1.2.1 会话中run方法

* run(fetches,feed_dict-None,graph=None)
  * 运行ops和计算tensor
  * 嵌套列表，元祖
    * namedtuple,dict或OrdereDict(重载的运算符也能运行)
  * feed_dict
    * 允许调用者覆盖图中指定张量的值，提供给placeholder使用
      * placeholder是一个占位符
        * plt = tf.placeholder(tf.float32,[2,3])
  * 返回值异常
    * RuntimeError：如果它Session处于无效状态（例如已关闭）
    * TypeError：如果fetches或feed_dict键是不适合的类型
    * ValueError：如果fetches或feed_dict键无效或引用Tensor不存在

## 1.3 张量(tensor)

### 1.3.1 基础

* numpy：ndarray类型
  * 数组 ： 一维
  * 矩阵：二维
  * 张量： 封装成tensor类型

* Tensorflow基本的数据格式
* 一个类型化的N维度数组（tf.Tensor)
* 三部分组成：名字（op类型）、形状（0/1/2...维）【专业术语：阶】、数据类型
  * graph：张量所属的默认图
  * op：张量的操作名
  * name：张量的字符串描述
  * shape：张量形状
    * 0维：()
    * 1维：(3)
    * 2维：(5,6)，不固定行：(？,6)
    * 3维：(2,3,4)

* 静态形状和动态形状

  ```python
  plt = tf.placeholder(tf.float32,[None,2])
  
  print(plt)
  
  plt.set_shape([3,2])   #已经设置为静态形状且张量形状固定，不能再次设置（不能跨维度修改）
  print(plt)
  
  plt_reshape = tf.reshape(plt,[2,3])  #动态形状修改（改变遵循元素数量匹配）
  print(plt_reshape)
  
  with tf.Session() as sess:
      pass
  ```

  

  > 在于有没有生成一个新的张量数据

  * 静态形状

    > 一旦张量形状固定，不能再次设置静态形状

    * 创建一个张量，初始化的形状
      * tf.Tensor.get_shape：获取静态形状
      * tf.Tensor.set_shape()：更新Tensor对象的静态形状

  * 动态形状

    > 动态形状可创建一个新张量（改变要遵循元素数量匹配）

    * 一种描述原始张量在执行过程中的一种形状（动态变化）
      * tf.reshape：创建一个具有不同动态形状的新张量

  * 要点

    * 转换静态形状时，1-D到1-D，2-D到2-D，不能跨阶数改变形状
    * 对于已经固定或者设置静态形状的张量/变量，不能再次设置静态形状
    * tf.reshape()动态创建新张量时，元素个数必须匹配

### 1.3.2 api

> [api接口](https://www.tensorflow.org/api_docs/python/tf)

**生成张量:**

* 固定值张量
  * **tf.zeros(shape,dtype=tf.float32,name=None)**
    * 创建所有元素设置为零的张量。此操作返回一个dtype具有形状shape和所有元素设置为零的类型的张量
  * tf.zeros_like(tensor,dtype=None,name=None)
    * 给tensor定单张量（），此操作返回tensor与所有元素设置为零相同的类型和形状的张量。
  * **tf.ones(shape,dtype=tf.float32,name=None)**
    * 创建一个所有元素设置为1的数量，此操作返回一个类型的张量，dtype形状shape和所有元素设置为1。
  * tf.ones_like(tensor,dtype=None,name=None)
    * 给tensor定单张量()，此操作返回tensor与所有元素设置为1相同的形状和张量。
  * tf.fill(dims,value,name=None)
    * 创建一个填充了标量值的张量，此操作创建一个张量形状dims并填充它value
  * **tf.constant(value,dtype=None,shape=None,name='Const')**
    * 创建一个常数张量
* 创建随机张量
  * tf.random_normal(shape,mean=0.0,stddev=1.0,dtype=tf.float32,seed=None,name=None)
    * mean：平均值
    * stddev：标准差
* 张量变换
  * 改变类型
    * tf.string_to_number(string_tensor,out_type=None,name=None)
    * tf.to_double(x,name='ToDouble')
    * ...
    * tf.cast(x,dype,name=None)
* 张量形状和变换
  * 用于确定张量的形状并更改张量的形状
    * tf.shape(imput,name=None)
    * tf.reshape(tensor,shape,name=None)

* 切片与扩展

  * tf.concat(values,axis,name='concat')

    * ```python
      a = [[1,2,3],[4,5,6]]
      b = [[7,8,9],[10,11,12]]
      c = tf.concat([a,b],axis=0)	# axis=0 按行合并
      ```

      

## 1.4 变量和模型

### 1.4.1 变量

> 1、变量也是一种OP，是一种特殊的张量，能够进行**存储持久化**，它的值就是张量，默认被训练
>
> 2、当定义一个变量op时，一定要在会话中运行初始化
>
> 3、name参数：在tensorboard使用的时候显示名字，可以让相同名字进行区分

* 创建变量
  * tf.Variable(initial_value=None,name=None,trainable=True)
    * 创建一个带值initial_value的新变量
    * assign(value)
      * 为变量分配一个新值，返回新值
    * eval(session=None)
      * 计算并返回此变量的值
    * name属性表示变量名字

# 2.可视化学习(Tensorboard)

> 程序图结构  ==序列化文件==》events事件文件 ==tensorboard==》web界面

## 2.1 开启tensorboard

* 数据序列化-events文件
  * Tensorboard通过读取TensorFlow的事件文件来运行
* tf.summary.FileWriter('./summary/',graph=)
  * 返回filewriter，写入事件到指定目录，以提供给tensorboard使用
* 开启
  * tensorboard  --logdir=’./summary/'
  * 127.0.0.1:6006
* **修改程序后，再保存一遍会有新的事件文件，打开默认为最新**

## 2.2 增加变量显示

> 目的：观察模型的参数、损失值等变量值的变化

* 1、收集变量
  * tf.summary.scalar(name='',tensor)
    * 收集对于损失函数和准确率等单值变量，name为变量名，tensor为值
  * tf.summary.histogram(name='',tensor)
    * 收集高纬度的变量参数
  * tf.summary.image(name='',tensor)
    * 收集输入的图片张量能显示图片
* 2、合并变量写入事件文件
  * merged = tf.summary.merge_all()
  * 运行合并：summary = sess.run(merged)
    * 每次迭代都需要运行
  * 添加：FileWriter.add_summary(summary,i)
    * i表示第几次的值

# 3.Tensorflow运算API

* 矩阵运算

  * tf.matmul(x,w)

* 平方

  * tf.square(error)

* 均值

  * tf.reduce_mean(error)

* 梯度下降

  * tf.train.GradientDescentOptimizer(learning_rate)

    * learning_rate：学习率

  * 方法

    * minimize(loss)：最小优化损失

  * return：梯度下降op

    

## 3.1 训练参数问题：trainable

* bias = tf.Variable(0.0,name='b',trainable=True)
  * 决定你在训练过程中参数是否可变

## 3.2 学习率和步数设置

- tf.train.GradientDescentOptimizer(0.1)

  - 学习率太大会导致训练结果误差太大
  - 同时也会影响到学习的步数

- 在极端情况下，权重的值变得非常大，以至于溢出，导致NaN值

  **解决问题**

- 1.重新设计网络

- 2.调整学习率

- 3.使用梯度截断（在训练过程中检查和限制梯度的大小）

- 4.使用激活函数

## 3.3 作用域

* tf.variable_scope(scope_name) 
  * 创建指定名字的变量作用域
* 嵌套使用变量作用域

## 3.4 增加变量显示

> 目的：观察模型的参数、损失值等变量值的改变

* 1.收集变量
  * tf.summary.scalar(name='',tensor)
    * 收集对于损失函数和准确率等单值变量，name为变量的名字，tensor为值
  * tf.summary.histogram(name='',tensor)
    * 收集高维度的变量参数
  * tf.summary.image(name='',tensor)
    * 收集输入的图片张量能显示图片
* 2.合并变量写入事件文件
  * merged = tf.summary.merge_all()
  * 运行合并：summary = sess.run(merged)
    * 每次迭代都需要运行
  * 添加：FileWriter.add_summary(summary,i)
    * i表示第几次的值

# 4. 模型保存和加载

* tf.train_Saver(var_list=None,max_to_keep=5)

  * var_list：指定将要保存和还原的变量。它可以作为一个dict或一个列表传递
  * max_to_keep：指示要保留的最近检查点文件的最大数量。
    * 创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件
    * 默认为5（即保留最新的5个检查点文件）

  * ```python
    #实例
    saver = tf.train_Saver(var_list=None,max_to_keep=5)
    saver.save(sess,'./../保存敏子')
    saver.restore(sess,'./..')
    ```

  * 保存文件格式：checkpoint文件

## 4.1 自定义命令行参数

* 1、tf.app.flags,它支持应用从命令行接受参数，可以用来指定集群配置等，在tf.app.flags下有各种定义参数的类型
  * DEFINE_string(flag_name,default_value,docstring)
  * DEFINE_integer(flag_name,default_value,docstring)
  * DEFINE_boolean(flag_name,default_value,docstring)
  * DEFINE_float(flag_name,default_value,docstring)
* 2、tf.app.flags，在flags有一个FLAGS标志，它在程序中可调用定义的flag_name
* 3、通过tf.app.run()启动main(argv)函数



# 5. 线程队列与IO操作

* 优化
  * 多线程：并行执行任务（无GIL）
  * 队列：读取和计算

## 5.1 队列和线程

* Tensorflow队列

  > 在训练样本，读入的训练样本有序

  * tf.FIFOQueue ：先进先出队列

    * ```markdown
      * FIFOQue(capacity,dtypes,name='fifo_queue')
      	* capacity：整数。可能存储在此队列中的元素数量的上限
      	* dtypes：DType对象列表。长度dtypes必须等于每个队列元素中的张量数，`dtype的类型形状，决定了后面队列元素形状`
      	
      * 方法
      	* dequeue(name=None)
      		* 出队列
      	* enqueue(vals,name=None)
      		* 入队列
      	* enqueue_many(vals,name=None)
      		* 入队列，vals列表或者元祖
      		* `注意：tensorflow默认把[]等当做张量，vals [,]用逗号隔开指定是列表，防止与tensor混淆`
          * size(name=None)
          	* 队列数据大小
      ```

  * tf.RandomShuffleQueue

    * 随机出队列

* 队列管理器

  * tf.train.QueueRunner(queue,enqueue_ops=None)

    > 创建一个QueueRunner

    * queue：一个队列
    * enqueue_ops：添加线程的队列操作列表，[]*2 指定两个线程

  * 方法

    * create_threads(sess,coord=None,start=False)

      > 创建线程来运行给定会话的入队操作

      * start：布尔值，True为启动线程，如果为False，调用者必须调用start()启动线程
      * coord：线程协调器，后面线程管理需要用到
      * return：线程的实例
    
  * 线程协调器
  
    * tf.train.Coordinator()
  
      > 线程协调员，实现一个简单的机制来协调一组线程的终止
  
      * request_stop()
        * 请求线程停止
      * should_stop()
        * 检查是否要求停止
      * join(threds=None,stop_grace_period_secs=120)
        * 等待线程终止
      * return：线程协调员实例

# 6.文件读取

## 6.1 流程介绍

* 1、构造文件队列
* 存放文件队列
  
* 2、读取队列内容：read
* csv文件：读取一行
  * 二进制文件：指定一个样本的bytes读取
  * 图片文件：按一张一张读取
  * 默认读取：默认读取一个样本

* 3、读取队列内容(解码)：decode
  * 一个队列的内容

* 4、批处理
  * 读取 
    * A文件样本1
    * ...
    * A文件样本n
    * B文件样本
* 5、主线程：取样本数据训练



## 6.2 文件读取API

* 1.文件队列构造

  * tf.train.string_input_producer(string_tensor,shuffle=True)

    > 将输出字符串（例如文件名）输入到管道队列

    * string_tensor：含有文件名的1阶张量
    * shuffle：是否顺序读取
    * num_epochs：过几遍数据，默认无限过数据
    * return：具有输出字符串的队列

* 2.文件阅读器

  > 不同的文件格式，选择对应的文件阅读器

  * tf.TextLineReader

    > 阅读文本文件逗号分隔值（csv）格式，默认按行读取

    * return：读取器实例

  * tf.FixedLengthRecorReader(record_bytes)

    > 要读取每个记录是固定数量字节的二进制文件

    * record_bytes：整型，指定每次读取的字节数
    * return：读取器实例

  * tf.TFRecordReader

    > 读取TfRecords文件

  * 共用的读取方法

    * read(file_queue)：从队列中指定数量内容
      * 返回一个Tensors元组（key文件名，value默认内容（行，字节））

* 3.文件内容解码器

  > 由于从文件中读取的是字符串，需要函数去解析这些字符串到张量

  * tf.decode_csv(records,record_defaults=None,field_delim=None,name=None)

    > 将CSV转换为张量，与tf.TextLineReader搭配使用

    * records：tensor型字符串，每个字符串是csv中记录行
    * field_delim：默认分割符 “,”
    * record_defaults：参数决定了所有张量的类型，并设置一个值在输入字符串中缺少使用默认值
    * return：有几列，就有几个返回值

  * tf.decode_raw(bytes,out_type,little_endian=None,name=None)

    > 将字节转换为一个数字向量表示，字节为一字符串类型的张量，与函数tf.FixedLengthRecordReader搭配使用，二进制读取为uint8格式

* 4.开启线程操作

  * tf.train.start_queue_runners(sess=None,coord=None)

    > 收集所有图中的队列线程，并启动线程

    * sess：所在的会话中
    * coord：线程协调器
    * return：返回所有线程队列

* 5.管道读端批处理

  * tf.train.batch(tensors,batch_size,num_threads=1,capacity=32,name=None)

    > 读取指定大小（个数）的张量

    * tensors：可以是包含张量的列表
    * batch_size：从队列中读取的批处理大小
    * num_threads：进入队列的线程数
    * capcity：整数，队列中元素的最大数量
    * return：tensors

  * tf.train.shuffle_batch(tensors,batch_size,capacity,min_after_dequeue,num_threads=1)

    > 乱序读取指定大小（个数）的张量

    * min_after_dequeue：留下队列里的张量个数，能保持随机打乱

# 7.图像读取

## 7.1 图像基本知识

> 所有图片统一特征数量（像素值一致）

* 图片(200×200)由像素组成

  * 长度
  * 宽度
  * 通道数
    * 单通道（黑白图片）

      * 灰度值 [0~255]
        * 一个像素点只有一个值
      * 特征：200 x 200 = 40000（个像素值）
    * 三通道（彩色图片）

      * RGB
        * 一个像素点由三个值组成
      * 特征：200 x 200 x 3  =  40000 *3  = 120000（个像素值）

* 张量表达形式
  
* [height,width,channels]
    * height：长度
  * width：宽度
    * channels：通道数

## 7.2 图像基本操作

* 目的

  * 增加图片数据的统一性
  * 所有图片转换成指定大小
  * 缩小图片数据量，防止增加开销

* 图像基本操作API

  * tf.image.resize_images(images,size)

    > 缩小图片

    * images：4-D形状的张量[batch,height,width,channels]

      ​				 3-D形状的张量[height,width,channels]的图片数据

      * batch：张数

    * size：1-D int32张量：new_height，new_width，图像的新尺寸

    * return：4-D格式或者3-D格式图片

## 7.3 图像读取API

> 存储：uint8（节省空间）
>
> 矩阵计算：float32（提高精度）

* 图像读取器

  * tf.WholeFileReader

    > 将文件的全部内容作为值输出的读取器

    * return：读取器实例
    * read(file_queue)：输出将是一个文件名（key）和该文件的内容（value）

* 图像解码器

  * tf.image.decode_jpeg(contents)

    > 将JPEG编码的图像解码为uint8张量

    * return：uint8张量，3-D形状[height,width,channels]

  * tf.image.decode_png(contents)

    > 将PNG编码的图像解码为uint8或uint16张量

    * return：张量类型，3-D形状[height,width,channels]



# 8. 二进制文件读取

> .bin格式
>
> 数据组成：<标签><3072像素>
* [CIFAR-10数据集](https://www.cs.toronto.edu/~kriz/cifar.html)
	*CIFAR-10数据集包含10个类别的60000个32*32彩色图像



# 9. TFRecords文件读取

> TFRecords是Tensorflow设置的一种内置文件格式，是一种二进制文件，.方便读取和移动
>
> **为了将二进制数据和标签（训练的类别标签）数据存储在同一个文件中**

* TFRecords文件分析
  * 文件格式： *.tfrecords
  * 写入文件内容：Example协议块（类字典的格式）

## 9.1 TFRecords存储

* **1、建立TFrecords存储器**

  * tf.python_io.TFRecordWriter(path)

    > 写入tfrecords文件
    >
    > **注：字符串为一个序列化的Example,Example.SerializeToString()**

    * path：TFRecords文件的路径
    * return：写文件

  * 方法

    * write(record)：向文件中写入一个example（字符串记录）

    * close()：关闭文件写入器

* **2、构造每个样本的Example协议块**
  
* 	tf.train.Example(features=None)
  
      > 写入tfrecords文件
    
      * features：train.Features类型的特征实例
      * return：example格式协议块
    
* tf.train.Features(feature=None)
  
  > 构造每个样本的信息键值对
  
  * feature：字典数据，key为要保存的名字
  
    ​									value为tf.train.Feature实例
  
  * return：Features类型
    
  * 	tf.train.Feature(**options)
  
     * 	**options事例如下
        * 	bytes_list = tf.train.BytesList(value=[Bytes])
        * 	int64_list = tf.train.Int64List(value=[Value])
        * 	float_list = tf.train.FloatList(value=[value])
  
## 9.2 TFRecods文件读取

> 同文件阅读器流程，中间需要解析过程
>
> 解析TFRecords的example协议内存块

* tf.parse_single_example(serialized,features=None,name=None)

  > 解析一个单一的Example原型

  * serialized：标量字符串Tensor，一个序列化的Example
  * features：dict字典数据，键为读取的名字，值为FixdLenFeature
  * return：一个键值对组成的字典，键为读取的名字

* tf.FixdLenFeature(shape,dtype)

  * shape：输入数据的形状，一般不指定，为空列表
  * dtype：输入数据类型，与存储文件类型要一致
    * **类型只能为float32,int64,string**

# 10.神经网络（多分类）  

* | 算法     | 策略         | 优化                     |
  | -------- | ------------ | ------------------------ |
  | 线性回归 | 均方误差     | 梯度下降                 |
  | 逻辑回归 | 对数似然损失 | 梯度下降                 |
  | 神经网络 | 交叉熵损失   | 反向传播算法（梯度下降） |

  * 正向传播：输出经过一层层的计算得出输出
  * 反向传播：从损失计算开始，梯度下降更新权重

*  感知机（解决分类问题）

  * ​	有n个输入数据，通过权重与各数据之间的计算和，比较激活函数结果，得出输出

* 定义

  * ```vim
    人工神经网络（artificial neural network 缩写ANN),简称神经网络）或类神经网络，是一种模仿生物网络的结构和功能的计算模型，用于对函数进行估计或近似
    ```

* 神经网络种类

  * 基础神经网络
    * 单层感知器、线性神经网络、BP神经网络、Hopfield神经网络等
  * 进阶神经网络
    * 玻尔兹曼机、受限玻尔兹曼机、递归神经网络等
  * 深度神经网络
    * 深度置信网络、卷积神经网络、循环神经网络、LSTM网络

* 特点

  * 输入向量的维度和输入神经元的个数相同
  * 每个连接都是有个权值
  * 同一层神经元之间没有连接
  * 有**输入层**，**隐层**，**输出层** 组成
  * 第N层与第N-1层的所有神经元连接，也叫全连接

* 组成
  * 结构（Architecture）：神经元网络中权重，神经元等等
  * 激活函数（Activity Rule）
  * 学习规则（Learning Rule）学习规则指定了网络中权重如何随着时间推进而调整（反向传播算法）

## 10.1 神经网络API模块

* tf.nn

  > 提供神经网络相关操作的支持，包括卷积操作(conv)，池化操作(pooling)，归一化，loss、分类操作、embedding、RNN、Evaluation

* tf.layers

  > 主要提供的高层神经网络，主要和卷积相关的，对tf.nn的进异步封装

* tf.contrib

  > tf.contrib.layers提供能够将计算图中的网络层、正则化、摘要操作、是构建计算图的高级操作，但tf.contrib包不稳定以及一些实验代码



* 全连接-从输入直接到输出

  > 特征加权

  * tf.matmul(a,b,name=None) + bias
    * return：全连接结果，供交叉损失运算
      * **注：**不需要激活函数（因为是最后输出）

* SoftMax计算、交叉熵

  * tf.nn.softmax_cross_entropy_with_logits(labels=None,logits=None,name=None)

    > 计算logits和labels之间的交叉损失熵

    * labels：标签值（真实值）
    * logits：样本加权之后的值
    * return：返回损失值列表

* 损失值列表平均值

  * tf.reduce_mean(input_tensor)

    > 计算张量的尺寸的元素平均值

* 损失下降API

  * tf.train.GradientDescentOptimizer(learning_rate)

    > 梯度下降优化

    * learning_rate：学习率
    * minimize(loss)：最小化损失
    * return：梯度下降op

* one-hot 

  * tf.one_hot(

    ​			indices,depth,on_value=None,

    ​			off_value=None,axis=None,

    ​			dtype=None,name=None

    )

    * indices：在读热编码中位置，即数据集标签

    * depth：张量的深度，即类别数

      * 实例

      * ```vim
        indices = [0,1,2,1]
        depth = 3
        
        [
        [1,0,0],
        [0,1,0],
        [0,0,1],
        [1,1,0]
        ]
        ```

* 准确率
  * equal_list = tf.equal( tf.argmax(y,1) , tf.argmax(y_label,1) )
  * accuracy = tf.reduce_mean(tf.cast(equal_list,tf.float32))

## Minst数据集神经网络

* Mnist手写数字识别
  * 训练数据集和测试数据集（xs，ys） 
  * 
* 获取数据
  * from tensorflow.examples.tutorials.mnist import input_data
  * mnist = input_data.read_data_sets(FLAGS.data_dir,one_hot=True)