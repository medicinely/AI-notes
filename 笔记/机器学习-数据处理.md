# 1.数据集

* **数据源**
  * scikit-learn
  * UCI
  * kaggle

* **数据集组成**
  * 数据集 = 特征值 + 目标值 



> 特征抽取对文本等数据进行特征值化

* 特征值化是为了计算机更好的去理解数据

# 2.特征抽取

## 2.1字典特征抽取

> 作用：对字典数据进行特征值化

```markdown
* sklearn.feature_extraction.DictVectorizer(sparse=True)
	* DictVectorizer.fit_transform(x)
		* x `字典或包含字典的迭代器`
		* 返回值：返回sparse矩阵
	* DictVectorizer.inverse_transform(x)
		* ｘ `array数组或者sparse矩阵`
		* 返回值　转换之前的数据格式
	* DictVectorizer.transform(x)
		* `按照先前的标准转换`
		

```

## 2.2 文本特征抽取

> 作用：对文本数据进行特征值化
>
> 代码案例：请移步到机器学习－>特征工程

* 1.统计所有文章当中所有的词，重复的只看做一次，（词列表）
* 2.对每篇文章，在词列表进行统计每个词出现的次数　

```markdown
* from sklearn.feature_extraction.text import CountVectorizer
	* `返回词频矩阵`
		* CountVectorizer.fit_transform(x)
			* x： 文本或者包含`文本字符串的可迭代对象`
			* 返回值：返回sparse矩阵
		* CountVectorizer.inverse_transform(x)
			* x：array数组或者sparse矩阵
			* 返回值：转换之前数据格式
		* CountVectorizer.get_feature_names()
			* 返回值：单词列表
```

## 2.3 文本分类

>  **在文本中许多中性词语（所以，我们，明天...）会影响到文章的类型区分**

### 2.3.1 TF-IDF

* 主要思想
  * 如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现
  * 则认为此词或者短语具有很好的类别区分能力，适用来分类
* TF-IDF作用
  * 用以评估一字词对一个文件集或一个语料库中的其中一份文件的`重要程度`



> tf：term frequency（词的频率）
>
> idf：inverse document frequency（逆文档频率）

* **idf公式：**

  * $$
    idf = log\frac{总文档数量}{该词出现的文档数量}
    $$

* **重要性程度**

  * $$
    重要性＝tf*idf
    $$

    

**TF-IDF中的api**

```markdown
* from sklearn.feature_extraction.text import TfidfVectorizer
	* TfidVectorizer(stop_words=None,...)
    	*返回词的权重矩阵
    		* TfidVectorizer.fit_transform(x)
    			* x：文本或者包含`文本字符串的可迭代对象`
    			* 返回值：返回sparse矩阵
    		* TfidVectorizer.inverse_transform(x)
    			* x：array数组或者sparse矩阵
    		* TfidVectorizer.get_feature_names()
    			* 返回值：单词列表
    			
```

## 2.4.数值性数据

### 2.4.1 数学基础

- **方差**

  > 反映了一组数据与其平均值的偏离程度

  - ```markdown
    实例 ：(1,2,3,4,5)
    ```

    $$
    \begin{cases} 
    mean=\frac{1+2+3+4+5}{5}=3 & \text mean为平均值\\
    S^2 = \frac{(1-3)^2+(2-3)^2+(3-3)^2+(4-3)^2+(5-3)^2}{5}  & \text S^2为方差
    \end{cases}
    $$

- **标准差**

  > 对方差开根号

  - $$
    \delta = \sqrt{S^2}  \qquad \delta ：标准差
    $$

- **平均差**

  > 表示各个变量值之间差异程度的数值

  - $$
    MD=\frac{\sum{|x-x'|}}{n} \qquad MD：平均差
    $$

  - ```
    实例：（2,3,4）
    ```

  - $$
    \begin{cases} 
    x'=\frac{2+3+4}{3}=3 & \text{x'算术平均值}\\
    MD = \frac{|2-3|+|3-3|+|4-3|}{3} & \text{MD为平均差}\\ 
    \end{cases}
    $$

  

  

### 2.4.2 归一化

> 易受到异常点影响

**前提：特征值同等重要**

**特点：通过对原始数据进行变换把数据映射到（默认[0,1]）之间**

**目的：使得一个特征对最终结果不会造成更大影响**



* **数学公式**

  * $$
    \begin{cases} 
    x' = \frac{x-min}{max-min} \\
x''=x'(mx-mi)+mi
    \end{cases}
    $$
    
    
    $$
    注：作用于每一列，max为一列的最大值，min为一列的最小值\\
    那么x''为最终结果，mx,mi分别指定区间值默认mx为1，mi为0
    $$
    

**1.归一化api**

```markdown
* from sklearn.preprocessing import MinMaxScaler(feature_range=(0,1)...)
	* 每个特征缩放到给定范围（默认[0,1]）
		* MinMaxScalar.fit_transform(x)
			* x：numpy array格式的数据[n_samples,n_features]
			* 返回值：转换后的形状相同的array
```

**2.总结**

* 在特定场景最大值最小值是变化的
* 最大值和最小值非常容易受异常点影响
  * 所以这种方法`鲁棒性较差`，只适合`传统精确小数据场景`
    * 鲁棒性：反应产品的稳定性

### 2.4.3 标准化

> 不易受到异常点影响
>
> 特点：通过对原始数据进行变换把数据变到均值为0，方差为1范围内

**1. 标准化计算**

  * $$
    \begin{cases} 
    x' =  \frac{x-mean}{\delta}  & \text{mean：平均值， x'：标准化结果}\\
\delta = \sqrt{S^2}  & \text{S^2：方差}
    \end{cases}
    $$
    
$$
注：方差：考量数据的稳定性
$$


​    

**2. 标准化api**

* ```markdown
  * from sklearn.preprocessing import StandardScaler
  	* 处理后每列来说：所有数据都聚集在`均值0附近，方差为1`
  	* StandardScaler.fit_transform(x)
  		* x：numpy array格式数据[n_samples,n_features]
  		* 返回值：转换后的形状相同的array
  	* StandardScaler.mean_
  		* 原始数据中每列特征的平均值
  	* StandardScaler.std_
  		* 原始数据每列特征的方差
  ```

  

**3.  总结**

* 在已有`样本足够多的情况下比较稳定`，适合现代嘈杂大数据场景

### 2.4.4 缺失值

| 删除 | 如果每列或者行数据缺失值达到一定的比例，建议放弃正整行或者整列 |
| ---- | ------------------------------------------------------------ |
| 插补 | 可以通过缺失值每行或者每列的平均值、中位数来填充             |

**缺失值api**

```markdown
* from sklearn.preprocessing import Imputer（
	missing_values='NaN',
	strategy='mean',
	axis=0,
	）
	* 参数说明
		* missing_values：操作某缺失值
		× strategy:操作方法，默认为平均值
		* axis：选择行或列，默认为列 axis=0
	*完成缺失值插补
		* Imputer.fit_transform(x)
			* x：numpy array格式的数据[n_samples,n_features]
			* 返回值：转换后的形状相同的array
```

### 2.4.5 时间类型

> 略

# 3. 数据降维(特征减少)

> **降维：把特征数量下降**

## 3.1 特征选择

* **特征选择原因**
  * 冗余：部分特征的相关度高，容易消耗计算性能
  * 噪声：部分特征对预测结果有影响
* **特征选择内容**
  * 特征选择就是单纯地从提取到`所有特征中选择部分特征`作为训练集特征
  * 特征在`选择前后可以对值产生改变`，但选择后的特征维数比选择前小
* **主要方法**
  * **Filter(过滤式)：VarianceThreshold**
  * **Embedded(嵌入式)：正则化、决策树**
  * Wrapper(包裹式)
  * **神经网络**

### 3.1.1 Filter(过滤式)

* **考虑因素**
  
* 方差大小：考虑所有样本这个特征的数据情况
  
* **sklearn特征选择API**

  * ```markdown
    * from sklearn.feature_selection import VarianceThreshold
    	*删除所有低方差特征
    	* Variance.fit_transform(x)
    		* x: numpy array格式的数据【n_samples,n_features】
    		* 返回值：`训练集差异低于threshold的特征将被删除`
    		* 默认值是保留所有非零方差特征 
    			* `即删除所有样本具有相同的特征`
    ```

## 3.2 主成分分析（PCA）

> 高维转换成低维

* **知识**

  * 本质：PCA是一种分析、简化数据集的技术
  * 目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息
  * 作用：**可以削减回归分析或者聚类分析中的特征数量**

* **场景**

  * PCA：特征数量达到上百的时候
    * 考虑数据的简化，数据也会改变，特征数量也会减少

* PCA语法

  * ```markdown
    * from sklearn.decomposition import PCA(n_components=None)
    	* n_components
    		* 小数形式：0-1  保留 90~100%的特征
    		* 整数形式：选择保留的特征
    	* 将数据分解为较低维数空间
    	* PCA.fit_transform(x)
    		* x : numpy array格式的数据【n_samples,n_features】
    		* 返回值：转换后指定维度的array
    ```




# 4.分类模型性能评估指标

> 面向对象：二元分类器
>
> 　业内常用的评价指标有准确率（Precision），召回率（Recall）、F值(F-Measure)等

## 4.1 混淆矩阵

> 在分类任务下，预测结果（Predicted Condition)与正确标记（True Condition）之间存在四种不同组合，构成混淆矩阵（适用于多分类）

​												

|              | 预测结果 | eg:是猫    | 不是猫     |
| ------------ | -------- | ---------- | ---------- |
| **真实结果** |          | 正例 P     | 假例 N     |
| eg：猫       | 正例 T   | 真正例ＴＰ | 伪反例ＦＮ |
| 不是猫       | 假例 F   | 伪正例ＦＰ | 真反例ＴＮ |

```markdown
* 准确率：`预测结果为正例`样本中真实为正例的比例（查得准）
	* 真实结果为：真正例ＴＰ
	* 预测结果为：　正例
* 召回率：`真实为正例`样本中预测结果为正例的比例（查的全,对正样本的区分能力)
	* 真实结果为：正例
	* 预测结果为：真正例ＴＰ
```

## 4.2 数学公式

- 准确率

  > **查的准**
  >
  > 缺点：例如地`震数据中（0：无地震，1：有地震）`，在大概率中测试样例被划分为0，小概率为1，`准确率可达99%`
  >
  > 当地震为1的发生，分类器将毫不察觉`（发生地震概率过小，不易察觉）`

  - $$
    ACC =\frac{TP+TN}{TP+TN+FP+FN}
    $$

    $$
    准确率 = \frac{提取出的正确信息条数}{提取出的信息条数}
    $$

    

- 精确率

  > 在预测概率中，预测成功的概率

  - $$
    P =	\frac{TP}{TP+FP}
    $$

- 召回率

  > **查的全**

  - $$
    R = \frac{TP}{TP+FN}
    $$

    $$
    召回率 = \frac{提取出的正确信息条数}{样本中的信息条数}
    $$

- F值(F-Measure)

  > 正确率和召回率的调和平均值

  $$
  F = \frac{(a^2+1)P*R}{a^2(P+R)}
  $$

  

  - $$
    \frac{正确率*召回率*2}{正确率+召回率}
    $$

    

## 4.3 实例

> 在一个鱼塘中，以`捕抓鲤鱼`为目的

| 鱼塘品种   | 总数量 | 抓取数量 |
| ---------- | ------ | -------- |
| 鲤鱼（条） | 1400   | 700      |
| 虾（只）   | 300    | 200      |
| 鳖（只）   | 300    | 100      |

```markdown
* 准确率：700/(700+200+100)= 70%
---
* 召回率：700/1400=50%
---
* F值：70% * 100% * 2 / (70% + 100%) = 82.35% 
---

准确率：评估捕获的成果中目标成果所占得比率
---
召回率：从关注领域中，召回目标类别的比例
---
F值：综合两者指标的评估指标，用于综合反映整体的指标
---

**极端情况：只捕获一条鲤鱼
准确率：100%
召回率：1/1400
× 需要根据具体的情况使用评估指标
```

​	

## 2.4 混淆矩阵api

```markdown
* sklearn.metrics.classification_report(y_true,y_pred,target_names=None)
	* y_true：真实目标值
	* y_pred：估计器预测目标值
	* target_names：目标类别名称
	* return：每个类别精确率与召回率

```

# 5. 模型的选择与调优

## 5.1 交叉验证

> 为了让被评估的模型更加准确可信

**训练集**：｛训练集，验证集｝

> 所有数据分成Ｎ等分

**４折交叉验证：**

|          |          | 训练模型 |          |        | 结果             |
| -------- | -------- | -------- | -------- | ------ | ---------------- |
| `验证集` | 训练集   | 训练集   | 训练集   | 模型１ |                  |
| 训练集   | `验证集` | 训练集   | 训练集   | 模型２ | 求平均值模型结果 |
| 训练集   | 训练集   | `验证集` | 训练集   | 模型３ |                  |
| 训练集   | 训练集   | 训练集   | `验证集` | 模型4  |                  |

## 5.2 网格搜索（超参数搜索）

> 通过**遍历**给定的参数组合来优化模型表现

- 通常情况下，有很多参数需要手动指定（如k-近邻算法中的k值）

  - 这种成为`超参数`，手动过程繁杂，所以需要对模型预设集中超参数组合
  - `每组超参数都采用交叉验证来评估`，最后选出最优参数组合

- 假定存在多个超参数 ：a[1,2,4,6]，b[2,4,6,7]

  - 俩俩组合：共有15种组合比较

  

## 5.3 网格搜索API

> 超参数搜索
>
> ​	sklearn.model_selection.GridSearchCV

```markdown
* sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
	* 对估计器的指定参数值进行详尽搜索
        * estimator：估计器对象
        * param_grid：估计器参数(dict){'n_neighbors':[1,3,5]}
        * cv：指定几折交叉验证
        * fit：输入训练数据
        * score：准确率
	* 分析结果
		* best_score_：在交叉验证中验证的最好结果
		* best_estimator_：最好的参数模型
		* cv_results_：每次交叉验证后的`验证集准确率结果`和`训练集准确率结果`
```

## 5.4.过拟合和欠拟合

> 主要看训练集的训练结果查看属于那种情况

- 过拟合

  - ```markdown
    一个假设`在训练数据上能够获得比其他假设更好的拟合`，但是在`训练数据外的数据集上却不能很好地拟合数据`，此时认为这个假设出现了过拟合的现象。（模型过于复杂）
    ```

  - 原因：原始特征过多，存在一些嘈杂特征，模型过于复杂是因为模型尝试去兼顾各个测试数据点

  - 解决：

    - 进行特征选择，消除关联性大的特征（很难做）
    - 交叉验证（让所有数据都有过训练）
    - 正则化（了解）

- 欠拟合

  - ```markdown
    一个假设`在训练数据上不能获得更好的拟合`，但是在`训练数据外的数据集上也不能很好地拟合数据`，此时认为这个假设出现了欠拟合的现象。（模型过于简单）
    ```

  - 原因：学习到数据特征过少

  - 解决：增加数据的特征数量



# 6.模型的保存与加载

- from sklearn.externals import joblib
  - 文件格式pkl（二进制文件）
  - 保存：joblib.dump(rf,'test.pkl')
  - 加载：estimator = joblib.load('test.pkl')



